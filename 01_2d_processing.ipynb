{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageXD 2019 - Images across domains\n",
    "\n",
    "BIDS @ University of California, Berkeley\n",
    "\n",
    "* Support material for the tutorial _scikit-image: 3D Image Processing_.\n",
    "\n",
    "This tutorial will introduce how to analyze three dimensional stacked and volumetric\n",
    "images in Python, mainly using scikit-image. Here we will learn how to:\n",
    "  * pre-process data using filtering, binarization and segmentation techniques.\n",
    "  * inspect, count and measure attributes of objects and regions of interest in the data.\n",
    "  * visualize 3D data.\n",
    "\n",
    "Please prepare for the tutorial by [installing the pre-requisite\n",
    "software](preparation.md) beforehand.\n",
    "\n",
    "For more info:\n",
    "  * [[ImageXD 2019]](https://xd-con.org/imagexd-2019/)\n",
    "  * [[scikit-image]](https://scikit-image.org/)\n",
    "\n",
    "\n",
    "## What is scikit-image?\n",
    "\n",
    "scikit-image is a collection of image processing algorithms which aims to integrate well with for the SciPy ecosystem.\n",
    "\n",
    "It is well documented, and provides well-tested code to quickly build sophisticated image processing pipelines.\n",
    "\n",
    "\n",
    "## Checking the system\n",
    "\n",
    "First, we'll check if your system have the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run check_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the base Scientific Python ecossystem\n",
    "\n",
    "Let's start importing the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supplementary_code as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's set a nice, `monospace` font for matplotlib's figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'monospace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to three-dimensional image processing\n",
    "\n",
    "In scikit-image, images are represented as `numpy` arrays.\n",
    "\n",
    "A grayscale image is a 2D matrix of pixel intensities of shape `(row, column)`. They are also called single-channel images. Multi-channel data has an extra dimension, `channel`, in the final position. `channel` contains color information. \n",
    "\n",
    "We can construct a 3D volume as a series of 2D `planes`, giving 3D images the shape `(plane, row, column)`.\n",
    "\n",
    "Summarizing:\n",
    "\n",
    "|Image type|Coordinates|\n",
    "|:---|:---|\n",
    "|2D grayscale|(row, column)|\n",
    "|2D multichannel|(row, column, channel)|\n",
    "|3D grayscale|(plane, row, column)|\n",
    "|3D multichannel|(plane, row, column, channel)|\n",
    "\n",
    "Some 3D images are constructed with equal resolution in each dimension. An example would be a computer generated rendering of a sphere with dimensions `(30, 30, 30)`: 30 planes, 30 rows and 30 columns.\n",
    "\n",
    "However, most experimental data captures one dimension at a lower resolution than the other two. For example, photographing thin slices to approximate a 3D structure as a stack of 2D images. We will work with one example of such data in this tutorial.\n",
    "\n",
    "\n",
    "## [skimage.io](https://scikit-image.org/docs/stable/api/skimage.io.html) - utilities to read and write images in various formats<a id='io'></a>\n",
    "\n",
    "This module helps us on reading images and saving the results. There are multiple plugins available, which support multiple formats. The most commonly used functions include:\n",
    "\n",
    "* `io.imread`: read an image to a numpy array.\n",
    "* `io.imsave`: write an image to disk.\n",
    "* `io.imread_collection`: read multiple images which match a common pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io  # skimage's I/O submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be loaded with `io.imread`, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = io.imread('data/cells.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check its shape, data type and range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* \"cells\" shape: {}'.format(cells.shape))\n",
    "print('* \"cells\" type: {}'.format(cells.dtype))\n",
    "print('* \"cells\" range: {}, {}'.format(cells.min(), cells.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `cells` has 60 planes, each with 256 rows and 256 columns. Let's focus here on processing one of the planes first, which falls to the case of processing a two-dimensional image. Then, we can visualize a 2D plane using `skimage.io.imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane = cells[32]  # using the plane 32\n",
    "io.imshow(plane, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `skimage.io.imshow` can only display grayscale and RGB(A) 2D images.\n",
    "\n",
    "## [skimage.exposure](https://scikit-image.org/docs/stable/api/skimage.exposure.html) - evaluating or changing the exposure of an image<a id='exposure'></a>\n",
    "\n",
    "This module contains a number of functions for adjusting image contrast. We will use `exposure.adjust_gamma`, which performs gamma correction in the input image.\n",
    "\n",
    "\n",
    "[Gamma correction](https://en.wikipedia.org/wiki/Gamma_correction), also known as Power Law Transform, brightens or darkens an image. The function $O = I^\\gamma$ is applied to each pixel in the image. A `gamma < 1` will brighten an image, while a `gamma > 1` will darken an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure  # skimage's exposure module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_val_low = 0.5\n",
    "plane_gamma_low = exposure.adjust_gamma(plane, gamma=gamma_val_low)\n",
    "\n",
    "gamma_val_high = 1.5\n",
    "plane_gamma_high = exposure.adjust_gamma(plane, gamma=gamma_val_high)\n",
    "\n",
    "_, ((win_top_left, win_top_center, win_top_right),\n",
    "    (win_bottom_left, win_bottom_center, win_bottom_right)) = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "# Original and its histogram.\n",
    "sc.show_plane(win_top_left, plane, title='Original')\n",
    "sc.plot_hist(win_bottom_left, plane)\n",
    "\n",
    "# Gamma = 0.5 and its histogram.\n",
    "sc.show_plane(win_top_center, plane_gamma_low, title='Gamma = {}'.format(gamma_val_low))\n",
    "sc.plot_hist(win_bottom_center, plane_gamma_low)\n",
    "\n",
    "# Gamma = 1.5 and its histogram.\n",
    "sc.show_plane(win_top_right, plane_gamma_high, title='Gamma = {}'.format(gamma_val_high))\n",
    "sc.plot_hist(win_bottom_right, plane_gamma_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most experimental images are affected by salt and pepper noise. A few bright artifacts can decrease the relative intensity of the pixels of interest.\n",
    "\n",
    "A simple way to improve contrast is to clip the pixel values on the lowest and highest extremes. Here we use `exposure.rescale_intensity` for that. Clipping the darkest and brightest 0.5% of pixels will increase the overall contrast of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = np.percentile(plane, q=(0.5, 99.5))\n",
    "\n",
    "plane_clipped = exposure.rescale_intensity(\n",
    "    plane,\n",
    "    in_range=(vmin, vmax), \n",
    "    out_range=np.float32\n",
    ")\n",
    "\n",
    "io.imshow(plane_clipped);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call our dataset `plane_rescaled` from now on. It will contain the plane version with clipped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_rescaled = plane_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "__Exercise: <font color='red'>(‚è∞ 7 min)</font>__ One of the most common tools to evaluate exposure is the *histogram*, which plots the number of points which have a certain value against the values in order from lowest (dark) to highest (light).\n",
    "\n",
    "A well-known tool for that is the [Histogram equalization](https://en.wikipedia.org/wiki/Histogram_equalization), which improves contrast in an image by redistributing pixel intensities. This operation may enhance background noise, since the most common pixel intensities are spread out, allowing areas of lower local contrast to gain a higher contrast. \n",
    "\n",
    "[Adaptive histogram equalization](https://en.wikipedia.org/wiki/Adaptive_histogram_equalization), on the other hand, is suitable for improving the local contrast and enhancing the definitions of edges in each region of an image. The adaptive method computes several histograms, each corresponding to a section of the image, and uses them to redistribute its lightness values. \n",
    "\n",
    "Now, there's some tasks for you:\n",
    "  * Process `plane`'s histogram using histogram equalization, given in `exposure.equalize_hist`, and its adaptive version, from `exposure.equalize_adapthist`.\n",
    "  * Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "\n",
    "# First, let's create a version using histogram equalization. \n",
    "plane_equalized = exposure.equalize_hist(...)\n",
    "\n",
    "# Now, a version using CLAHE. \n",
    "plane_clahe = exposure.equalize_adapthist(...)\n",
    "\n",
    "# Let's check the results.\n",
    "_, ((win_top_left, win_top_center, win_top_right),\n",
    "    (win_bottom_left, win_bottom_center, win_bottom_right)) = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "# On the top, the 2D plots...\n",
    "sc.show_plane(win_top_left, ..., title='Original')\n",
    "sc.show_plane(win_top_center, ..., title='Histogram equalization')\n",
    "sc.show_plane(win_top_right, ..., title='CLAHE')\n",
    "\n",
    "# ... on the bottom, the histograms.\n",
    "sc.plot_hist(win_bottom_left, ...)\n",
    "sc.plot_hist(win_bottom_center, ...)\n",
    "sc.plot_hist(win_bottom_right, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection\n",
    "\n",
    "[Edge detection](https://en.wikipedia.org/wiki/Edge_detection) highlights regions in the image where a sharp change in contrast occurs. The intensity of an edge corresponds to the steepness of the transition from one intensity to another. A gradual shift from bright to dark intensity results in a dim edge. An abrupt shift results in a bright edge.\n",
    "\n",
    "## [skimage.filters](https://scikit-image.org/docs/stable/api/skimage.filters.html) - apply filters to an image<a id='filters'></a>\n",
    "\n",
    "Filtering applies whole-image modifications such as sharpening or blurring. In addition to edge detection, `skimage.filters` provides functions for filtering and thresholding images.\n",
    "\n",
    "Notable functions include (links to relevant gallery examples):\n",
    "\n",
    "* [Thresholding](https://scikit-image.org/docs/stable/auto_examples/applications/plot_thresholding.html):\n",
    "  * `filters.threshold_*` (multiple different functions with this prefix)\n",
    "  * `filters.try_all_threshold` to compare various methods\n",
    "* [Edge finding/enhancement](https://scikit-image.org/docs/stable/auto_examples/edges/plot_edge_filter.html):\n",
    "  * `filters.sobel` - not adapted for 3D images. It can be applied planewise to approximate a 3D result.\n",
    "  * `filters.prewitt`\n",
    "  * `filters.scharr`\n",
    "  * `filters.roberts`\n",
    "  * `filters.laplace`\n",
    "  * `filters.hessian`\n",
    "* [Ridge filters](https://scikit-image.org/docs/stable/auto_examples/edges/plot_ridge_filter.html):\n",
    "  * `filters.meijering`\n",
    "  * `filters.sato`\n",
    "  * `filters.frangi`\n",
    "* Inverse filtering (see also [skimage.restoration](#restoration)):\n",
    "  * `filters.weiner`\n",
    "  * `filters.inverse`\n",
    "* [Directional](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_gabor.html): `filters.gabor`\n",
    "* Blurring/denoising\n",
    "  * `filters.gaussian`\n",
    "  * `filters.median`\n",
    "* [Sharpening](https://scikit-image.org/docs/stable/auto_examples/filters/plot_unsharp_mask.html): `filters.unsharp_mask`\n",
    "* Define your own filter: `LPIFilter2D`\n",
    "  \n",
    "The sub-submodule `skimage.filters.rank` contains rank filters. These filters are nonlinear and operate on the local histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters  # skimage's filtering module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator) is an edge detection algorithm which approximates the gradient of the image intensity, and is fast to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_sobel = filters.sobel(plane)\n",
    "    \n",
    "io.imshow(plane_sobel, cmap='viridis')  # changing the map to ease visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "__Exercise: <font color='red'>(‚è∞ 7 min)</font>__ let's check the results of other filters.\n",
    "\n",
    "Your tasks right now are:\n",
    "  * Try the horizontal (`filters.sobel_h`) and vertical(`filters.sobel_v`) versions of the Sobel filter.\n",
    "  * Check the results of other filters, `filters.roberts`, `filters.prewitt`, `filters.scharr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "\n",
    "plane_sobel_h = ...\n",
    "plane_sobel_v = ...\n",
    "plane_roberts = ...\n",
    "plane_prewitt = ...\n",
    "plane_scharr = ...\n",
    "\n",
    "# Checking the results.\n",
    "_, ((win_top_left, win_top_center, win_top_right),\n",
    "    (win_bottom_left, win_bottom_center, win_bottom_right)) = plt.subplots(nrows=2, ncols=3, figsize=(16, 14))\n",
    "\n",
    "sc.show_plane(win_top_left, ..., title='Original')\n",
    "sc.show_plane(win_top_center, ..., title='Horizontal Sobel')\n",
    "sc.show_plane(win_top_right, ..., title='Vertical Sobel')\n",
    "\n",
    "sc.show_plane(win_bottom_left, ..., title='Roberts')\n",
    "sc.show_plane(win_bottom_center, ..., title='Prewitt')\n",
    "sc.show_plane(win_bottom_right, ..., title='Scharr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- POINT TO CONTINUE -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "[Gaussian filter](https://en.wikipedia.org/wiki/Gaussian_filter) applies a Gaussian function to an image, creating a smoothing effect. `skimage.filters.gaussian` takes as input `sigma` which can be a scalar or a sequence of scalar. This `sigma` determines the standard deviation of the Gaussian along each axis. When the resolution in the `plane` dimension is much worse than the `row` and `column` dimensions, dividing `base_sigma` by the image `spacing` will balance the contribution to the filter along each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sigma = 2.0\n",
    "sigma = base_sigma / spacing\n",
    "\n",
    "cells_gaussian = filters.gaussian(cells_rescaled, multichannel=False, sigma=sigma)\n",
    "\n",
    "sc.slice_explorer(cells_gaussian);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Median filter](https://en.wikipedia.org/wiki/Median_filter) is a noise removal filter. It is particularly effective against salt and pepper noise. An additional feature of the median filter is its ability to preserve edges. This is helpful in segmentation because the original shape of regions of interest will be preserved.\n",
    "\n",
    "`skimage.filters.median` does not support three-dimensional images and needs to be applied planewise.\n",
    "\n",
    "## [skimage.util](https://scikit-image.org/docs/stable/api/skimage.util.html) - utility functions<a id='util'></a>\n",
    "\n",
    "These are generally useful functions which have no definite other place in the package.\n",
    "\n",
    "* `util.img_as_*` are convenience functions for datatype conversion.\n",
    "\n",
    "* `util.invert` is a convenient way to invert any image, accounting for its datatype.\n",
    "\n",
    "* `util.random_noise` is a comprehensive function to apply any amount of many different types of noise to images.  The seed may be set, resulting in pseudo-random noise for testing.\n",
    "\n",
    "* `util.view_as_*` allows for overlapping views into the same memory array, which is useful for elegant local computations with minimal memory impact.\n",
    "\n",
    "* `util.apply_parallel` uses Dask to apply a function across subsections of an image.  This can result in dramatic performance or memory improvements, but depending on the algorithm edge effects or lack of knowledge of the remainder of the image may result in unexpected results.\n",
    "\n",
    "* `util.pad` and `util.crop` pads or crops the edges of images.  `util.pad` is now a direct wrapper for `numpy.pad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util  # skimage's util submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_rescaled_ubyte = util.img_as_ubyte(cells_rescaled)\n",
    "\n",
    "cells_median = np.empty_like(cells_rescaled_ubyte)\n",
    "\n",
    "for plane, image in enumerate(cells_rescaled_ubyte):\n",
    "    cells_median[plane] = filters.median(image)\n",
    "    \n",
    "cells_median = util.img_as_float(cells_median)\n",
    "    \n",
    "sc.slice_explorer(cells_median);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.restoration](https://scikit-image.org/docs/stable/api/skimage.restoration.html) - restoration of an image<a id='restoration'></a>\n",
    "\n",
    "This submodule includes routines to restore images.  Currently these routines fall into four major categories.  Links lead to topical gallery examples.\n",
    "\n",
    "* `restoration.denoise_*` - [Reducing noise](https://scikit-image.org/docs/stable/auto_examples/filters/plot_denoise.html).\n",
    "* [Deconvolution](https://scikit-image.org/docs/stable/auto_examples/filters/plot_deconvolution.html), or reversing a convolutional effect which applies to the entire image. This can be done in an [unsupervised](https://scikit-image.org/docs/stable/auto_examples/filters/plot_restoration.html) way.\n",
    "  * `restoration.weiner`\n",
    "  * `restoration.unsupervised_weiner`\n",
    "  * `restoration.richardson_lucy`\n",
    "* `restoration.inpaint_biharmonic` - [Inpainting](https://scikit-image.org/docs/stable/auto_examples/filters/plot_inpaint.html), or filling in missing areas of an image.\n",
    "* `restoration.unwrap_phase` - [Phase unwrapping](https://scikit-image.org/docs/stable/auto_examples/filters/plot_phase_unwrap.html).\n",
    "\n",
    "A [bilateral filter](https://en.wikipedia.org/wiki/Bilateral_filter) is another edge-preserving, denoising filter. Each pixel is assigned a weighted average based on neighboring pixels. The weight is determined by spatial and radiometric similarity (e.g., distance between two colors).\n",
    "\n",
    "`skimage.restoration.denoise_bilateral` requires a `multichannel` parameter. This determines whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension. While the function does not yet support 3D data, the `multichannel` parameter will help distinguish multichannel 2D data from grayscale 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import restoration  # skimage's restoration submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_bilateral = np.empty_like(cells_rescaled)\n",
    "\n",
    "for plane, image in enumerate(cells_rescaled):\n",
    "    cells_bilateral[plane] = restoration.denoise_bilateral(\n",
    "        image, \n",
    "        multichannel=False\n",
    "    )\n",
    "\n",
    "sc.slice_explorer(cells_bilateral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ((win_top_left, win_top_right),\n",
    "    (win_bottom_left, win_bottom_right)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "\n",
    "sc.show_plane(win_top_left, cells_rescaled[32], title='Original')\n",
    "sc.show_plane(win_top_right, cells_gaussian[32], title='Gaussian')\n",
    "sc.show_plane(win_bottom_left, cells_bilateral[32], title='Bilateral')\n",
    "sc.show_plane(win_bottom_right, cells_median[32], title='Median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_denoised = cells_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? üôÑ)</font>__ let's filter `beadpack_rescaled` now.\n",
    "\n",
    "Your tasks are:\n",
    "  * Use Gaussian, median and bilateral filters on `beadpack_rescaled`.\n",
    "  * Check the results; choose one and call it `beadpack_denoised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "sigma = \n",
    "\n",
    "# The Gaussian...\n",
    "beadpack_gaussian = filters.gaussian()\n",
    "\n",
    "sc.slice_explorer(gaussian);\n",
    "\n",
    "# ... the median...\n",
    "beadpack_rescaled_ubyte = util.img_as_ubyte()\n",
    "beadpack_median = np.empty_like()\n",
    "\n",
    "for plane, image in enumerate(beadpack_rescaled_ubyte):\n",
    "    beadpack_median[plane] = filters.median()\n",
    "    \n",
    "beadpack_median = util.img_as_float(beadpack_median)\n",
    "    \n",
    "sc.slice_explorer(beadpack_median);\n",
    "\n",
    "# ... and the bilateral filters.\n",
    "beadpack_bilateral = np.empty_like()\n",
    "\n",
    "for plane, image in enumerate():\n",
    "    beadpack_bilateral[plane] = restoration.denoise_bilateral(\n",
    "        , \n",
    "        multichannel=False\n",
    "    )\n",
    "\n",
    "sc.slice_explorer(beadpack_bilateral);\n",
    "\n",
    "# Choose your destiny!\n",
    "beadpack_denoised = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "[Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to create binary images. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker. Thresholding is a form of image segmentation.\n",
    "\n",
    "Different thresholding algorithms produce different results. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and Li's minimum cross entropy threshold are two common algorithms. The example below demonstrates how a small difference in the threshold value can visibly alter the binarized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_li = filters.threshold_li(cells_denoised)\n",
    "cells_binary_li = cells_denoised >= threshold_li\n",
    "\n",
    "threshold_otsu = filters.threshold_otsu(cells_denoised)\n",
    "cells_binary_otsu = cells_denoised >= threshold_otsu\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "\n",
    "sc.show_plane(win_left, cells_binary_li[32], title='Li\\'s threshold = {:0.2}'.format(threshold_li))\n",
    "sc.show_plane(win_center, cells_binary_otsu[32], title='Otsu\\'s threshold = {:0.2}'.format(threshold_otsu))\n",
    "\n",
    "sc.plot_hist(win_right, cells_denoised, 'Thresholds (Li: red, Otsu: blue)')\n",
    "win_right.axvline(threshold_li, c='r')\n",
    "win_right.axvline(threshold_otsu, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_binary = cells_binary_li\n",
    "\n",
    "sc.slice_explorer(cells_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? üôÑ)</font>__ let's binarize `beadpack_denoised`, but using different tools!\n",
    "\n",
    "Your tasks are:\n",
    "  * Use the function `filters.try_all_threshold` to check the binary version of the 100th plane of `beadpack_denoised`.\n",
    "  * Choose one of the thresholds, apply it on the data and call it `beadpack_binary`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "filters.try_all_threshold()\n",
    "\n",
    "threshold = filters.threshold_\n",
    "beadpack_binary = beadpack_denoised >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='morphology'></a>[skimage.morphology](https://scikit-image.org/docs/stable/api/skimage.morphology.html) - binary and grayscale morphology\n",
    "\n",
    "Morphological image processing is a collection of non-linear operations related to the shape or morphology of features in an image, such as boundaries, skeletons, etc. In any given technique, we probe an image with a small shape or template called a structuring element, which defines the region of interest or neighborhood around a pixel.\n",
    "\n",
    "[Mathematical morphology](https://en.wikipedia.org/wiki/Mathematical_morphology) operations and structuring elements are defined in `skimage.morphology`. Structuring elements are shapes which define areas over which an operation is applied. The response to the filter indicates how well the neighborhood corresponds to the structuring element's shape.\n",
    "\n",
    "There are a number of two and three dimensional structuring elements defined in `skimage.morphology`. Not all 2D structuring element have a 3D counterpart. The simplest and most commonly used structuring elements are the `disk`/`ball` and `square`/`cube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology  # skimage's morphological submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball = morphology.ball(radius=5)\n",
    "print('* Ball shape: {}'.format(ball.shape))\n",
    "\n",
    "cube = morphology.cube(width=5)\n",
    "print('* Cube shape: {}'.format(cube.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic mathematical morphology operations are `dilation` and `erosion`. Dilation enlarges bright regions and shrinks dark regions. Erosion shrinks bright regions and enlarges dark regions. Other morphological operations are composed of `dilation` and `erosion`.\n",
    "\n",
    "The `closing` of an image is defined as a `dilation` followed by an `erosion`. Closing can remove small dark spots (i.e. ‚Äúpepper‚Äù) and connect small bright cracks. This tends to ‚Äúclose‚Äù up (dark) gaps between (bright) features. Morphological `opening` on an image is defined as an `erosion` followed by a `dilation`. Opening can remove small bright spots (i.e. ‚Äúsalt‚Äù) and connect small dark cracks. This tends to ‚Äúopen‚Äù up (dark) gaps between (bright) features.\n",
    "\n",
    "These operations in `skimage.morphology` are compatible with 3D images and structuring elements. A 2D structuring element cannot be applied to a 3D image, nor can a 3D structuring element be applied to a 2D image.\n",
    "\n",
    "These four operations (`closing`, `dilation`, `erosion`, `opening`) have binary counterparts which are faster to compute than the grayscale algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = morphology.ball(radius=3)\n",
    "\n",
    "cells_closing = morphology.closing(cells_rescaled, selem=selem)\n",
    "cells_dilation = morphology.dilation(cells_rescaled, selem=selem)\n",
    "cells_erosion = morphology.erosion(cells_rescaled, selem=selem)\n",
    "cells_opening = morphology.opening(cells_rescaled, selem=selem)\n",
    "\n",
    "cells_binary_closing = morphology.binary_closing(cells_binary, selem=selem)\n",
    "cells_binary_dilation = morphology.binary_dilation(cells_binary, selem=selem)\n",
    "cells_binary_erosion = morphology.binary_erosion(cells_binary, selem=selem)\n",
    "cells_binary_opening = morphology.binary_opening(cells_binary, selem=selem)\n",
    "\n",
    "_, ((win_top_1, win_top_2, win_top_3, win_top_4),\n",
    "    (win_bottom_1, win_bottom_2, win_bottom_3, win_bottom_4)) = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "\n",
    "sc.show_plane(win_top_1, cells_erosion[32], title='Erosion')\n",
    "sc.show_plane(win_top_2, cells_dilation[32], title='Dilation')\n",
    "sc.show_plane(win_top_3, cells_closing[32], title='Closing')\n",
    "sc.show_plane(win_top_4, cells_opening[32], title='Opening')\n",
    "\n",
    "sc.show_plane(win_bottom_1, cells_binary_erosion[32], title='Binary erosion')\n",
    "sc.show_plane(win_bottom_2, cells_binary_dilation[32], title='Binary dilation')\n",
    "sc.show_plane(win_bottom_3, cells_binary_closing[32], title='Binary closing')\n",
    "sc.show_plane(win_bottom_4, cells_binary_opening[32], title='Binary opening')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphology operations can be chained together to denoise an image. For example, a `closing` applied to an `opening` can remove salt and pepper noise from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_binary_equalized = cells_equalized >= filters.threshold_li(cells_equalized)\n",
    "\n",
    "cells_despeckled_radius1 = morphology.closing(\n",
    "    morphology.opening(cells_binary_equalized, selem=morphology.ball(1)),\n",
    "    selem=morphology.ball(1)\n",
    ")\n",
    "\n",
    "cells_despeckled_radius3 = morphology.closing(\n",
    "    morphology.opening(cells_binary_equalized, selem=morphology.ball(3)),\n",
    "    selem=morphology.ball(3)\n",
    ")\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
    "\n",
    "sc.show_plane(win_left, cells_binary_equalized[32], title='Noisy data')\n",
    "sc.show_plane(win_center, cells_despeckled_radius1[32], title='Despeckled, r = 1')\n",
    "sc.show_plane(win_right, cells_despeckled_radius3[32], title='Despeckled, r = 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions operating on [connected components](https://en.wikipedia.org/wiki/Connected_space) can remove small undesired elements while preserving larger shapes.\n",
    "\n",
    "`skimage.morphology.remove_small_holes` fills holes and `skimage.morphology.remove_small_objects` removes bright regions. Both functions accept a `min_size` parameter, which is the minimum size (in pixels) of accepted holes or objects. The `min_size` can be approximated by a cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 20\n",
    "\n",
    "cells_remove_holes = morphology.remove_small_holes(\n",
    "    cells_binary,\n",
    "    width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_remove_holes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 20\n",
    "\n",
    "cells_remove_objects = morphology.remove_small_objects(\n",
    "    cells_remove_holes, \n",
    "    min_size=width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_remove_objects);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? üôÑ)</font>__ let's perform some operations on `beadpack_binary` and check the results.\n",
    "\n",
    "Your tasks are:\n",
    "  * Apply opening, closing, dilation and erosion on `beadpack_binary`.\n",
    "  * Generate binary histogram-equalized and CLAHE versions of `beadpack`, according to the threshold you chose previously.\n",
    "  * Remove small holes and objects on `beadpack_binary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "selem = morphology.ball()\n",
    "\n",
    "beadpack_binary_erosion = morphology.binary_erosion()\n",
    "beadpack_binary_dilation = morphology.binary_dilation()\n",
    "beadpack_binary_closing = morphology.binary_closing()\n",
    "beadpack_binary_opening = morphology.binary_opening()\n",
    "\n",
    "_, (win_1, win_2, win_3, win_4) = plt.subplots(nrows=1, ncols=4, figsize=(16, 5))\n",
    "\n",
    "sc.show_plane(win_bottom_1, , title='Binary erosion')\n",
    "sc.show_plane(win_bottom_2, , title='Binary dilation')\n",
    "sc.show_plane(win_bottom_3, , title='Binary closing')\n",
    "sc.show_plane(win_bottom_4, , title='Binary opening')\n",
    "\n",
    "beadpack_binary_equalized = beadpack_equalized >= filters.threshold_\n",
    "beadpack_binary_clahe = beadpack_clahe >= filters.threshold_\n",
    "\n",
    "width = 20\n",
    "\n",
    "beadpack_remove_holes = morphology.remove_small_holes(\n",
    "    ,\n",
    "    width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_remove_holes);\n",
    "\n",
    "beadpack_remove_objects = morphology.remove_small_objects(\n",
    "    , \n",
    "    min_size=width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_remove_objects);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='measure'></a>[skimage.measure](https://scikit-image.org/docs/stable/api/skimage.measure.html) - measuring image or region properties\n",
    "\n",
    "Multiple algorithms to label images, or obtain information about discrete regions of an image.\n",
    "\n",
    "* `measure.label` - Label an image, i.e. identify discrete regions in the image using unique integers.\n",
    "* `measure.regionprops` - In a labeled image, as returned by `label`, find various properties of the labeled regions.\n",
    "\n",
    "Finding paths from a 2D image, or isosurfaces from a 3D image.\n",
    "\n",
    "* `measure.find_contours`\n",
    "* `measure.marching_cubes_lewiner`\n",
    "* `measure.marching_cubes_classic`\n",
    "* `measure.mesh_surface_area` - Surface area of 3D mesh from marching cubes.\n",
    "* `measure.compare_*` - Quantify the difference between two whole images; often used in denoising or restoration.\n",
    "\n",
    "**RANDom Sample Consensus fitting (RANSAC)** - a powerful, robust approach to fitting a model to data.  It exists here because its initial use was for fitting shapes, but it can also fit transforms.\n",
    "* `measure.ransac`\n",
    "* `measure.CircleModel`\n",
    "* `measure.EllipseModel`\n",
    "* `measure.LineModelND`\n",
    "\n",
    "[Image segmentation](https://en.wikipedia.org/wiki/Image_segmentation) partitions images into regions of interest. Integer labels are assigned to each region to distinguish regions of interest.\n",
    "\n",
    "Connected components of the binary image are assigned the same label via `skimage.measure.label`. Tightly packed cells  connected in the binary image are assigned the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure  # skimage's measure submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_labels = measure.label(cells_remove_objects)\n",
    "\n",
    "sc.slice_explorer(cells_labels, cmap='nipy_spectral');\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(win_left, cells_rescaled[32, :100, 125:], title='Rescaled')\n",
    "sc.show_plane(win_center, cells_labels[32, :100, 125:], cmap='nipy_spectral', title='Labels')\n",
    "sc.show_plane(win_right, cells_labels[32, :100, 125:] == 8, title='Labels = 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better segmentation would assign different labels to disjoint regions in the original image. \n",
    "\n",
    "[Watershed segmentation](https://en.wikipedia.org/wiki/Watershed_%28image_processing%29) can distinguish touching objects. Markers are placed at local minima and expanded outward until there is a collision with markers from another region. The inverse intensity image transforms bright cell regions into basins which should be filled.\n",
    "\n",
    "In declumping, markers are generated from the distance function. Points furthest from an edge have the highest intensity and should be identified as markers using `skimage.feature.peak_local_max`. Regions with pinch points should be assigned multiple markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_distance = ndimage.distance_transform_edt(cells_remove_objects)\n",
    "\n",
    "sc.slice_explorer(cells_distance, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.feature](https://scikit-image.org/docs/stable/api/skimage.feature.html) - extract features from an image<a id='feature'></a>\n",
    "\n",
    "This submodule presents a diverse set of tools to identify or extract certain features from images, including tools for\n",
    "\n",
    "* Edge detection: `feature.canny`\n",
    "* Corner detection:\n",
    "  * `feature.corner_kitchen_rosenfeld`\n",
    "  * `feature.corner_harris`\n",
    "  * `feature.corner_shi_tomasi`\n",
    "  * `feature.corner_foerstner`\n",
    "  * `feature.subpix`\n",
    "  * `feature.corner_moravec`\n",
    "  * `feature.corner_fast`\n",
    "  * `feature.corner_orientations`\n",
    "* Blob detection\n",
    "  * `feature.blob_dog`\n",
    "  * `feature.blob_doh`\n",
    "  * `feature.blob_log`\n",
    "* Texture\n",
    "  * `feature.greycomatrix`\n",
    "  * `feature.greycoprops`\n",
    "  * `feature.local_binary_pattern`\n",
    "  * `feature.multiblock_lbp`\n",
    "* Peak finding: `feature.peak_local_max`\n",
    "* Object detction\n",
    "  * `feature.hog`\n",
    "  * `feature.match_template`\n",
    "* Stereoscopic depth estimation: `feature.daisy`\n",
    "* Feature matching\n",
    "  * `feature.ORB`\n",
    "  * `feature.BRIEF`\n",
    "  * `feature.CENSURE`\n",
    "  * `feature.match_descriptors`\n",
    "  * `feature.plot_matches`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature  # skimage's feature submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_local_max = feature.peak_local_max(\n",
    "    cells_distance,\n",
    "    footprint=np.ones((15, 15, 15), dtype=np.bool),\n",
    "    indices=False,\n",
    "    labels=measure.label(cells_remove_objects)\n",
    ")\n",
    "\n",
    "cells_markers = measure.label(peak_local_max)\n",
    "\n",
    "cells_labels = morphology.watershed(\n",
    "    cells_rescaled, \n",
    "    cells_markers, \n",
    "    mask=cells_remove_objects\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_labels, cmap='nipy_spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After watershed, we have better disambiguation between internal cells.\n",
    "\n",
    "When cells simultaneous touch the border of the image, they may be assigned the same label.  In pre-processing, we typically remove these cells.\n",
    "\n",
    "**Note:** This is 3D data -- you may not always be able to see connections in 2D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (win_left, win_right) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "sc.show_plane(win_left, cells_labels[39, 156:, 20:150], cmap='nipy_spectral')\n",
    "sc.show_plane(win_right, cells_labels[34, 90:190, 126:], cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The watershed algorithm falsely detected subregions in a few cells. This is referred to as oversegmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axis = plt.subplots()\n",
    "sc.show_plane(axis, cells_labels[38, 50:100, 20:100], cmap='nipy_spectral', title='Oversegmented labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the markers on the distance image reveals the reason for oversegmentation. Cells with multiple markers will be assigned multiple labels, and oversegmented. It can be observed that cells with a uniformly increasing distance map are assigned a single marker near their center. Cells with uneven distance maps are assigned multiple markers, indicating the presence of multiple local maxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 12))\n",
    "\n",
    "vmin = cells_distance.min()\n",
    "vmax = cells_distance.max()\n",
    "\n",
    "offset = 31\n",
    "\n",
    "for index, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(\n",
    "        cells_distance[offset + index],\n",
    "        cmap='gray',\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    \n",
    "    peaks = np.nonzero(peak_local_max[offset + index])\n",
    "    \n",
    "    ax.plot(peaks[1], peaks[0], 'r.')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 8))\n",
    "\n",
    "\n",
    "sc.show_plane(win_left, cells_remove_objects[10:, 193:253, 74])\n",
    "sc.show_plane(win_center, cells_distance[10:, 193:253, 74])\n",
    "\n",
    "features = feature.peak_local_max(cells_distance[10:, 193:253, 74])\n",
    "win_center.plot(features[:, 1], features[:, 0], 'r.')\n",
    "\n",
    "# Improve feature selection by blurring, using a larger footprint\n",
    "# in `peak_local_max`, etc.\n",
    "\n",
    "smooth_distance = filters.gaussian(cells_distance[10:, 193:253, 74], sigma=5)\n",
    "sc.show_plane(win_right, smooth_distance)\n",
    "features = feature.peak_local_max(\n",
    "    smooth_distance\n",
    ")\n",
    "win_right.plot(features[:, 1], features[:, 0], 'bx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? üôÑ)</font>__ now it's time to label `beadpack_remove_objects` and separate the beads!\n",
    "\n",
    "Your tasks are:\n",
    "  * Label `beadpack_remove_objects` using `measure.label`, and obtain the distance between the pixels.\n",
    "  * Try different footprints and obtain its max local peaks for `morphology.watershed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beadpack_labels = measure.label()\n",
    "\n",
    "sc.slice_explorer(beadpack_labels, cmap='nipy_spectral');\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(win_left, , title='Rescaled')\n",
    "sc.show_plane(win_center, , cmap='nipy_spectral', title='Labels')\n",
    "sc.show_plane(win_right, , title='Labels = 100')\n",
    "\n",
    "beadpack_distance = ndimage.distance_transform_edt()\n",
    "\n",
    "sc.slice_explorer(, cmap='magma');\n",
    "\n",
    "footprint =\n",
    "\n",
    "peak_local_max = feature.peak_local_max(\n",
    "    ,\n",
    "    footprint=, dtype=np.bool),\n",
    "    indices=False,\n",
    "    labels=measure.label(beadpack_remove_objects)\n",
    ")\n",
    "\n",
    "beadpack_markers = measure.label(peak_local_max)\n",
    "\n",
    "beadpack_labels = morphology.watershed(\n",
    "    beadpack_rescaled, \n",
    "    beadpack_markers, \n",
    "    mask=beadpack_remove_objects\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_labels, cmap='nipy_spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='segmentation'></a>[skimage.segmentation](https://scikit-image.org/docs/stable/api/skimage.segmentation.html) - identification of regions of interest\n",
    "\n",
    "One of the key image analysis tasks is identifying regions of interest.  These could be a person, an object, certain features of an animal, microscopic image, or stars.  Segmenting an image is the process of determining where these things you want are in your images.\n",
    "\n",
    "Segmentation has two overarching categories:\n",
    "\n",
    "**Supervised** - must provide some guidance (seed points or initial conditions)\n",
    "\n",
    "* `segmentation.random_walker`\n",
    "* `segmentation.active_contour`\n",
    "* `segmentation.watershed`\n",
    "* `segmentation.flood_fill`\n",
    "* `segmentation.flood`\n",
    "\n",
    "**Unsupervised** - no human input\n",
    "\n",
    "* `segmentation.slic`\n",
    "* `segmentation.felzenszwalb`\n",
    "* `segmentation.chan_vese`\n",
    "\n",
    "There are also some supervised and unsupervised thresholding algorithms in `filters`. There is a [segmentation lecture](https://github.com/scikit-image/skimage-tutorials/blob/master/lectures/4_segmentation.ipynb) ([and its solution](https://github.com/scikit-image/skimage-tutorials/blob/master/lectures/solutions/4_segmentation.ipynb)) you may peruse, as well as many [gallery examples](https://scikit-image.org/docs/stable/auto_examples/index.html#segmentation-of-objects) which illustrate all of these segmentation methods.\n",
    "\n",
    "[Feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) reduces data required to describe an image or objects by measuring informative features. These include features such as area or volume, bounding boxes, and intensity statistics.\n",
    "\n",
    "Before measuring objects, it helps to clear objects from the image border. Measurements should only be collected for objects entirely contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation  # skimage's segmentation submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_labels_inner = segmentation.clear_border(cells_labels)\n",
    "cells_labels_inner = morphology.remove_small_objects(cells_labels_inner, min_size=200)\n",
    "\n",
    "print('Interior labels: {}'.format(np.unique(cells_labels_inner)))\n",
    "\n",
    "sc.slice_explorer(cells_labels_inner, cmap='nipy_spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clearing the border, the object labels are no longer sequentially increasing. The labels can be renumbered such that there are no jumps in the list of image labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_relabeled, _, _ = segmentation.relabel_sequential(cells_labels_inner)\n",
    "\n",
    "print('Relabeled labels: {}'.format(np.unique(cells_relabeled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skimage.measure.regionprops` automatically measures many labeled image features. Optionally, an `intensity_image` can be supplied and intensity features are extracted per object. It's good practice to make measurements on the original image.\n",
    "\n",
    "Not all properties are supported for 3D data. Below are lists of supported and unsupported 3D measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = measure.regionprops(cells_relabeled, intensity_image=cells)\n",
    "props_first_region = properties[0]\n",
    "\n",
    "supported = [''] \n",
    "unsupported = ['']\n",
    "\n",
    "for prop in props_first_region:\n",
    "    try:\n",
    "        props_first_region[prop]\n",
    "        supported.append(prop)\n",
    "    except NotImplementedError:\n",
    "        unsupported.append(prop)\n",
    "\n",
    "print('Supported properties:')\n",
    "print('\\n\\t'.join(supported))\n",
    "print()\n",
    "print('Unsupported properties:')\n",
    "print('\\n\\t'.join(unsupported))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skimage.measure.regionprops` ignores the 0 label, which represents the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Measured regions: {}'.format([prop.label for prop in properties]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_volumes = [prop.area for prop in properties]\n",
    "\n",
    "print('Total pixels: {}'.format(cells_volumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collected measurements can be further reduced by computing per-image statistics such as total, minimum, maximum, mean, and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Volume statistics\\n')\n",
    "print(' * Total: {}'.format(np.sum(cells_volumes)))\n",
    "print(' * Min: {}'.format(np.min(cells_volumes)))\n",
    "print(' * Max: {}'.format(np.max(cells_volumes)))\n",
    "print(' * Mean: {:0.2f}'.format(np.mean(cells_volumes)))\n",
    "print(' * Standard deviation: {:0.2f}'.format(np.std(cells_volumes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? üôÑ)</font>__ let's clean the beads and prepare them to visualization!\n",
    "\n",
    "Here are your tasks:\n",
    "  * Clear the borders and remove small objects on `beadpack_labels`.\n",
    "  * Show the volume information for the beads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beadpack_labels_inner = segmentation.clear_border()\n",
    "beadpack_labels_inner = morphology.remove_small_objects()\n",
    "\n",
    "print('Interior labels: {}'.format(np.unique()))\n",
    "\n",
    "sc.slice_explorer(beadpack_labels_inner, cmap='nipy_spectral');\n",
    "\n",
    "beadpack_relabeled, _, _ = segmentation.relabel_sequential(beadpack_labels_inner)\n",
    "\n",
    "print('Relabeled labels: {}'.format(np.unique(beadpack_relabeled)))\n",
    "\n",
    "beadpack_volumes = [prop.area for prop in properties]\n",
    "\n",
    "print('total pixels: {}'.format(beadpack_volumes))\n",
    "\n",
    "print('Volume statistics\\n')\n",
    "print(' * Total: {}'.format(np.sum(beadpack_volumes)))\n",
    "print(' * Min: {}'.format(np.min(beadpack_volumes)))\n",
    "print(' * Max: {}'.format(np.max(beadpack_volumes)))\n",
    "print(' * Mean: {:0.2f}'.format(np.mean(beadpack_volumes)))\n",
    "print(' * Standard deviation: {:0.2f}'.format(np.std(beadpack_volumes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê BONUS! ‚≠ê‚≠ê Parallelizing image loops\n",
    "\n",
    "In image processing, we frequently apply the same algorithm on a large batch of images. Some of these image loops can take a while to be processed. Here we'll see how to use `joblib` to parallelize loops.\n",
    "\n",
    "Our bilateral application during this tutorial, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilateral_classic_loop():\n",
    "    cells_bilateral = np.empty_like(cells_rescaled)\n",
    "    for plane, image in enumerate(cells_rescaled):\n",
    "        cells_bilateral[plane] = restoration.denoise_bilateral(image, multichannel=False)\n",
    "    return cells_bilateral\n",
    "\n",
    "%timeit bilateral_classic_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert this loop to a `joblib` one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# when using n_jobs=-2, all CPUs but one are used.\n",
    "\n",
    "def bilateral_joblib_loop():\n",
    "    cells_bilateral = Parallel(n_jobs=-2)(delayed(restoration.denoise_bilateral)(image) for image in cells_rescaled)\n",
    "\n",
    "%timeit bilateral_joblib_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going beyond\n",
    "\n",
    "[1] A tour/guide on scikit-image's submodules: https://github.com/scikit-image/skimage-tutorials/blob/master/lectures/tour_of_skimage.ipynb\n",
    "\n",
    "[2] scikit-image's gallery examples: https://scikit-image.org/docs/stable/auto_examples/\n",
    "\n",
    "[3] ITK's `ikwidgets`: https://github.com/InsightSoftwareConsortium/itkwidgets\n",
    "\n",
    "[4] `joblib.Parallel`: https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
